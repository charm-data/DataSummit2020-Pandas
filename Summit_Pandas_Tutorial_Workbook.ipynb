{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020 RVATech/DataSummit Python Workshop\n",
    "### Introduction to Data Analysis with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. File I/O and working with data in pandas\n",
    "    * Reading, writing, and creating structured data in Python\n",
    "    * Viewing, inspecting, and selecting data\n",
    "2. Exploratory data analysis with pandas\n",
    "    * Data cleaning\n",
    "    * Summary statistics\n",
    "    * Data transformations\n",
    "    * Sorting, aggregation, joins and pivots\n",
    "    * Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/CSSEGISandData/COVID-19\n",
    "!awk '(NR == 1) || (FNR > 1)' COVID-19/csse_covid_19_data/csse_covid_19_daily_reports/0[12]*.csv > covid_19_data.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. File I/O and working with data in pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Python way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening a file and reading the content of a file is one of the common things you would do while doing data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to read all lines of a file at the same time, Python’s **readlines()** function is for you. Python’s readlines function reads everything in the text file and has them in a list of lines. Here is an example of how to use Python’s readlines.\n",
    "\n",
    "We first open the file using **open()** function in read only mode. And use the file handler from opening the file to read all lines using readlines() as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with read only permit\n",
    "f = open('covid_19_data.csv', \"r\")\n",
    "\n",
    "# use readlines to read all lines in the file\n",
    "# The variable \"lines\" is a list containing all lines in the file\n",
    "lines = f.readlines()\n",
    "\n",
    "# close the file after reading the lines.\n",
    "f.close() \n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read all the lines of a file at once in another way. Basically, we would use the file handler object after opening the file as argument to **list()** function to get all the lines as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all lines at once\n",
    "f = open('covid_19_data.csv', \"r\")\n",
    "lines = list(f)\n",
    "f.close() \n",
    "lines[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the last character of each line is newline character.\n",
    "\n",
    "Then you can go over the list of “lines” to parse each line. As you can immediately notice, “readlines” or “list(f) works great for a small text file. However, it is not memory efficient to use if your text files are really big. A better way to read a text file that is memory-friendly is to read the file line by line, that is one line at a time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the way to read text file one line at a time using “While” statement and python’s readline function. Since we read one line at a time with readline, we can easily handle big files without worrying about memory problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file with read only permit\n",
    "f = open('covid_19_data.csv')\n",
    "\n",
    "# use readline() to read the first line \n",
    "line = f.readline()\n",
    "\n",
    "# use the read line to read further.\n",
    "# If the file is not empty keep reading one line\n",
    "# at a time, till the file is empty\n",
    "line_number = 0\n",
    "while line:\n",
    "    if line_number < 10:\n",
    "        print(line)\n",
    "    line_number += 1\n",
    "    # use readline() to read next line\n",
    "    line = f.readline()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variation of reading a file with while statement and readline statement is as follows. Here the while tests for boolean and read line by line until we reach the end of file and line will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file handle fh\n",
    "fh = open('covid_19_data.csv')\n",
    "line_number = 0\n",
    "while True:\n",
    "    # read line\n",
    "    line = fh.readline()\n",
    "    \n",
    "    if line_number < 10:\n",
    "        print(line)\n",
    "    line_number += 1\n",
    "    \n",
    "    # check if line is not empty\n",
    "    if not line:\n",
    "        break\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can also use an iterator to read a text file one line at time. Here is how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fh = open('covid_19_data.csv')\n",
    "line_number = 0\n",
    "for line in fh:\n",
    "    if line_number < 10:\n",
    "        print(line)\n",
    "    line_number += 1\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering to close the file handler (“fh”) with the statement “fh.close()” can be difficult initially. One can check if a file handler is closed with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if the file file handler is closed or not\n",
    "fh.closed\n",
    "# true if the file handler is closed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial is to learn the most popular Python library for data analysis, **pandas**. We will learn how to work with existing data, as well as how to create your own data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is an open source library which is built on top of NumPy library, and which allows for fast analysis, data cleaning, and preparation of data efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To start working with pandas, we need to import this library:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Most of the time, you will be probably working with data that already exists in variety of different formats.\n",
    "By far the most basic of these are CSV or Excel files. This is how you read these files with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = pd.read_csv('covid_19_data.csv')\n",
    "covid['ObservationDate'] = pd.to_datetime(covid['Last Update']).map(lambda x: x.date()).map(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pause here for a minute, and try to understand what exactly happens once we run **pandas.read_csv** command.\n",
    "\n",
    "Two core objects in pandas are the **DataFrame** and the **Series**.\n",
    "\n",
    "A DataFrame is a table. It contains an array of individual entries, each of which has a certain value. Each entry corresponds to a row and a column. The list of row labels used in a DataFrame is known as an Index.\n",
    "\n",
    "In our example above, *covid* is a DataFrame created from 'covid_19_data.csv' file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can examine the contents of our DataFrame using the **head()** command:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this example, we can see that DataFrame entries are not limited to integers. For instance, the \"0, Province?State\" entry has the value of \"Beijing\", which is a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Series, the second core pandas object, is a sequence of data values. If a DataFrame is a table, a Series is a list and,  in essence, it is a single column of a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how we can see all the Series names in our DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The pd.read_csv() function has over 30 optional parameters you can specify. To learn more about them, run this line of code:\n",
    "#?pd.read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also get the same information by running help on any function:\n",
    "#help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, here are list of commands you can use to import different data formats:\n",
    "\n",
    "**pd.read_excel(filename)** - import from an Excel file\n",
    "\n",
    "**pd.read_sql(query, connection_object)** - read from a SQL table/database\n",
    "\n",
    "**pd.read_html(url)** - parses an html URL, string, or file and extracts tables to a s list of DataFrames\n",
    "\n",
    "**pd.read_json(json_string)** - read from a JSON formatted string, URL, or file\n",
    "\n",
    "**pd.read_clipboard()** - takes the contents of your clipboard and passes it to  *read_table()*\n",
    "\n",
    "**pd.read_table(filename)** - import from an delimited text file (like TSV)\n",
    "\n",
    "The [Pandas Documentation on file I/O](https://pandas.pydata.org/pandas-docs/stable/reference/io.html) also has a wealth of additional information on reading and writing various file types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating your own data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes you might need to be able to create a DataFrame or Series by hand.\n",
    "\n",
    "The standard way of constructing a new DataFrame is by using the *pd.DataFrame()* constructor, where you provide a dictionary whose keys are the column names, and whose values are a list of entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your own DataFrame:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, this method assigns values to the column labels, and uses and ascending count (0, 1, 2,..) fro the row labels. If, instead, we want to assign labels ourselves, we can do it by adding an *index* option to DataFrame contructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'Country': ['China', 'Italy', 'Iran'], 'Known Cases': ['80000', '2000' , '1500']}, \\\n",
    "#             index = ['Observation 1', 'Observation 2', 'Observation 3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a Series by providing a list of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create your own Series:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or:\n",
    "#pd.Series(['Asia', 'Europe', 'North America', 'Australia/Oceania', 'South America', 'Antarctica', 'Africa'],\\\n",
    "#          name = 'Continents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to *pd.read_csv()*, we can use *to_csv()* method when we need to export our dataframe to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's give a name to our  dataframe, and save it as a CSV file:\n",
    "#observations = pd.DataFrame({'Country': ['China', 'Italy', 'Iran'], 'Known Cases': ['80000', '2000' , '1500']}, \\\n",
    "#             index = ['Observation 1', 'Observation 2', 'Observation 3'])\n",
    "#observations.to_csv(\"Observations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can save the same dataframe without headers and index:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also save to a specified location:\n",
    "#observations.to_csv(r'C:\\Desktop\\Observations.csv', index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another popular format for output files is Excel. To export pandas dataframe to an Excel file, we can run a *to_excel()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save 'observations' DataFrame to Excel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To specify the tab name:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we have a need to export the results of our data analysis to more than one sheet in the workbook. In such case, it is necessary to specify an ExcelWriter object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#continents = pd.Series(['Asia', 'Europe', 'North America', 'Australia/Oceania', \\\n",
    "#                        'South America', 'Antarctica', 'Africa'], name = 'Continents')\n",
    "#with pd.ExcelWriter('Continents_Observations.xlsx') as writer:  \n",
    "#    continents.to_excel(writer, sheet_name='Continents')\n",
    "#    observations.to_excel(writer, sheet_name='Observations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just for the reference,  here is a couple of alternative methods to export data:\n",
    "\n",
    "**your_dataframe_name.to_sql(table_name, connection_object)** - write to a SQL table\n",
    "\n",
    "**your_dataframe_name.to_json(filename)** - write to a file in JSON format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing, inspecting, and selecting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to our **covid** DataFrame. Remember - you can use *head()* method to view the first *n* rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first 3 rows of data set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In a similar way, you can inspect last n rows with tail() method:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas objects have a number of attributes enabling you to access the metadata. We can also use the **shape** attribute to check how large this DataFrame is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the data set size:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that our DataFrame has 2818 records split across 8 different columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see list of all columns, use *columns* attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the list of all DataFrame columns:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to access values in any column within the DataFrame. One is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at first 5 rows of 'ObservationDate' column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And another way is to use Python indexing (**[ ]**) operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at first 5 rows of 'Last Update' column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(The advantage of this way is that it handles column names with reserved characters in them.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also select multiple columns by providing list of their names (this will return columns as a new DataFrame):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid[['ObservationDate', 'Confirmed', 'Recovered']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look at a single specific value within Series, we need to use the indexing operator once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid['Country/Region'][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to Python-native indexing operator, pandas has its own operators, **loc** and **loc**. \n",
    "\n",
    "We use **iloc** to select data based on its numerical position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select first row of data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If, instead, we want to get first column of data, we would use:\n",
    "#covid.iloc[:, 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use **loc** for label-based selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.loc[:, ['ObservationDate', 'Country/Region', 'Confirmed']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, data selection based on *conditions* provides the most interesting insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at Italy-related data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select 'Italy' Country/Region data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the number of confirmed cases in Italy as of February 29, we can use the apmersand (**&**) to bring the two questions together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confirmed cases in Italy as of February 29, 2020:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If, instead, we are interested in seeing all the data related to California or Washington states in US, we would use a pipe (**|**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid[(covid['Country/Region'] == 'US') &\n",
    "      ((covid['Province/State'] == 'Washington') | (covid['Province/State'] == 'California'))].head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the same results, we might also use pandas built-in conditional selector **isin()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative way of looking at all data relates to CA and WA states in US:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we might want to re-write all the values within the Series. For example, we can create a new column \"Source\", and fill it in with \"Trusted\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new \"Source\" column:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory data analysis with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove unnecessary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have covered some basics of data exporting and viewing, let's see how we can clean our sample dataset for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, let's delete the \"Source\" column we just created. To do this, we will use **drop()** method. Its *axis* parameter identifies whether we want to drop labels from the index (0 or ‘index’) or columns (1 or ‘columns’):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid = covid.drop(['Source', 'Last Update'], axis = 1)\n",
    "#covid.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Here, we also dropped 'Last Update' column since it was showing time in UTC at which the row is updated for the given province or country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, data sets we are working with come with column names or index names which are not very convenient. For example, we have 'Province/State' and 'Country/Region' columns which have special characters in their names, and also 'Last Update' column with a space in it. Let's rename them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename 'Province/State' column to a 'Region', and 'Country/Region' column to a 'Country':\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicated records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we check if there are any duplicated records usinf **duplicated()** method which returns boolean True/False values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.duplicated().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course we do not want to scrool through thousands of returned True/False. Instead, we can check for unique vaules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicated values using unique() method:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See actual duplicated records:\n",
    "#covid[covid.duplicated() == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove duplicates:\n",
    "covid = covid.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove duplicated records from the DataFrame, we would use the following syntax:\n",
    "\n",
    "**df = df.drop_duplicates()**\n",
    "\n",
    "To remove duplicates of only one or a subset of columns, we can specify *subset* as the individual column or list of columns that should be unique:\n",
    "\n",
    "**df.drop_duplicates(subset = ['Column_1', 'Column_2'], keep = 'False')**\n",
    "\n",
    "We can do the same task conditional on a different column’s value. In such case we can **sort_values()** first, and specify **keep** equals either first or last:\n",
    "\n",
    "**df.sort_values('Column_1', ascending=False)**\n",
    "\n",
    "**df = df.drop_duplicates(subset='Column_2', keep='first')**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also investigate data types within our dataframe. To do this, we use **dtype** (for a Series) or **dtypes** (for a DataFrame) attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.Confirmed.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that columns consisting entirely of strings do not get their own type in pandas; they are instead given the object type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we would like to convert a column of one type into another wherever such a conversion makes sense by using the astype() function. In our dataframe, we may transform the *Deaths* column from its existing float64 data type into a int64 data type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'Deaths' values to integers:\n",
    "covid.Deaths = covid.Deaths.fillna(0).astype('int64')\n",
    "covid.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, to make work with dates easier, let's convert 'ObservationDate' column to pandas *datetime* object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert ObservationDate column to datetime:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert strings to uppercase:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times, when we work with string data in our data set, it might be a good idea to convert all characters to uppercase and strip extra whitespaces before and after each string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify strings within the 'Region' and 'Country' Series of our data set. To convert strings in the Series/Index to uppercase, we can use **upper()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert 'Region' and 'Country' to uppercase:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove leading and trailing characters, we would use **strip()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid.Region = covid.Region.str.strip()\n",
    "#covid.Country = covid.Country.str.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Sometimes, we might want to remove leading (**lstrip()**) or trailing (**rstrip()**) characters only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, when we work with real-world data, we always have some missing records. To find these missing values, also known as **NaN** (Not a Number), in our data set, we would use **isnull** method (or, alternatively, **notnull()** method which will select not empty values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check which coumns in our data set have missing values:\n",
    "#covid.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid[covid.Region.isnull()].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform data analysis, we would like to replace missing values, and pandas provides a really handy method for this problem: **fillna()**. \n",
    "\n",
    "In our case, we can simply replace each NaN with an 'Unknown':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing 'Region' calues with 'Inknown':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explore missing 'Confirmed' values:\n",
    "#covid[covid.Confirmed.isnull()].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing  'Confirmes' values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it makes sense to remove missing values from data set, and in that case we would use **dropna()** method. By default, **dropna()** will drop all rows in which any null value is present:\n",
    "\n",
    "**df.dropna()**\n",
    "\n",
    "We can also drop missing values along a different axis; *axis=1* drops all columns containing a null value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides some handy methods which allow us to see a high-level summary of data. We already familiar with one of these methods - **describe()**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied to numerical data:\n",
    "#covid.Confirmed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied to string data:\n",
    "#covid.Country.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the reference, here are some others commonly used methods (they can be applies to a Series as well):\n",
    "\n",
    "**df.describe()** - summary statistics for numerical columns\n",
    "\n",
    "**df.mean()** - returns the mean of all columns\n",
    "\n",
    "**df.corr()** - returns the correlation between columns in a DataFrame\n",
    "\n",
    "**df.count()** - returns the number of non-null values in each DataFrame column\n",
    "\n",
    "**df.max()** - returns the highest value in each column\n",
    "\n",
    "**df.min()** - returns the lowest value in each column\n",
    "\n",
    "**df.median()** - returns the median of each column\n",
    "\n",
    "**df.std()** - returns the standard deviation of each column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of all unique values in certain Series, we would use the **unique()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See the list of unique 'Country' values:\n",
    "#covid.Country.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nunique()** will return a number of unique values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of unique countries:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data transformations with 'map' and 'apply'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two methods in pandas which allow us to take one set of values and \"map\" them to another set. **map()** returns a new Series where all the values have been transformed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have noticed that 'Country' Series contains, among others, entries for 'Mainland China', 'Hong Kong', and 'Macau'. We know that they are all parts of People's Republic of China, and would prefer to use 'China' for all of them. Here is how this can be done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = set(['MAINLAND CHINA', 'HONG KONG', 'MACAU'])\n",
    "#covid['GeneralCountry'] = covid.Country.map(lambda x: 'CHINA' if x in s else x)\n",
    "#covid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we used a small anonimous function (*lambda function*) which can take any number of arguments, but can only have one expression. Once the expression is executed, it returns the result.\n",
    "\n",
    "In our example, this lambda function takes a single value from the 'Country' Series, and return a transformed version of that value. Then **map()** returns a new Series (which we called 'GeneralCountry' where all the values have been transformed by our labmda function.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**apply()** is a similar method which be applied both to Series and DataFrames. The difference is that **apply()** works on a row/column basis of a DataFrame, while **map()** works element-wise on a Series.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function which calculates \"naive\" mortality rate, and write its output in a new 'MortalityRate' column using **apply()** method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create MortalityRate column using apply() method:\n",
    "#def naive_rate(x):\n",
    "#    if x.Confirmed > 0:\n",
    "#        return x.Deaths/x.Confirmed\n",
    "#    else:\n",
    "#        return 0\n",
    "#covid['MortalityRate'] = covid.apply(naive_rate, axis = 1)\n",
    "#covid[covid.MortalityRate != 0].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases, we would like to first sort values in our data set, and then perform further data manipulations. To sort data and get it in the desired order we can use **sort_values()** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort values by 'GeneralCountry':\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sort_values()** defaults to an ascending sort, where the lowest values go first. To obtain a descending sort, we need to specify *ascending* parameter: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort values by 'ObservationDate' and 'Confirmed' in descending order:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on our goals, we might prefer to group data first, and then perform some operations on this group. In pandas, we can do it with **groupby()** method. However, simple call of a *groupby* method on our data set will return not a set of DataFrames, but a DataFrameGroupBy object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by 'GeneralCountry' and create a DataFrameGroupBy object:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of this object as of a special view of the DataFrame, which is already divided into groups. To get actual results, we need to apply an aggregate on this object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example, we can check when the date of latest observation for each country:\n",
    "#covid.groupby('GeneralCountry').ObservationDate.max().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can use any of the summary functions with groupby object. For example, we can see latest numbers of confirmed cases within various regions of China:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#covid[covid.GeneralCountry == 'CHINA'].groupby('Region').Confirmed.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also group by more than one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by 'GeneralCountry' and 'Region' and see maximum (=total) number of 'Deaths':\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of regions within each country:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See 'Confirmed' and 'Deaths' numbers by GeneralCountry:\n",
    "#covid.groupby('GeneralCountry')[['Confirmed', 'Deaths']].max().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reset_index()** method allows to reset index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reset index in just created dataframe:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often enough, while analyzing data, we need to combine different DataFrames and/or Series in non-trivial ways. Pandas has a few neat methods for doing this. \n",
    "\n",
    "In cases when we have data in different DataFrames (or Series) but having the same fields (columns), **concat()** function lets us join dataframes along axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we create a new DataFrame using *groupby()* method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countries_stats = covid.groupby('GeneralCountry')[['Confirmed', 'Deaths']].max().reset_index()\n",
    "#countries_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And pretend we have just heard of two new confirmed cases - one in Mongolia, and one in Jamaica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_cases = pd.DataFrame({'GeneralCountry': ['MONGOLIA', 'JAMAICA'], 'Confirmed': [1.0, 1.0], 'Deaths': [0, 0]})\n",
    "#new_cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we would like to study all of the affected countries simultaneously, we would use **concat()** method to stack these two DataFrames on top of one another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate counties_stats and new_cases in new 'all_cases' DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check:\n",
    "#all_cases[all_cases.GeneralCountry == 'JAMAICA']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To concat the same two DataFrames along columns, we would switch *axis* parameter to 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the same DataFrames along  columns:\n",
    "#all_cases_too = pd.concat([countries_stats, new_cases], axis = 1)\n",
    "#all_cases_too.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merge()** or **join()** methods let you combine different DataFrame objects which have an index in common (similar to a JOIN in SQL)\n",
    "\n",
    "Let's create a new DataFrame listing countries and latest observation dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new DataFrame with countries and latest observation dates:\n",
    "#latest_data = covid.groupby(['GeneralCountry', 'Region']).ObservationDate.max().reset_index()\n",
    "#latest_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to join this DataFrame with our original 'covid' DataFrame to get latest fatality rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#latest_rates = pd.merge(latest_data, covid[['GeneralCountry', 'Region', 'ObservationDate', 'MortalityRate']], \\\n",
    "                        on=['GeneralCountry', 'Region', 'ObservationDate'])\n",
    "#latest_rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify if we need inner, outer, right, or left join, as well as add a suffix to duplicate column names:\n",
    "\n",
    "**pd.merge(df_1, df_2, on='ID', how='left', suffixes=('_1', '_2'))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With pandas, we can also create a spreadsheet-style pivot table as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an Excel-style pivot table:\n",
    "#pd.pivot_table(covid[['GeneralCountry', 'Region', 'ObservationDate', 'Confirmed']], \\\n",
    "#               index=['GeneralCountry','Region'], aggfunc='max')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualizations with pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to quickly view how our data looks, pandas visualizations come in handy.\n",
    "\n",
    "Let's look again at *all_cases* DataFrame we created some time ago:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at first 5 rows of 'all_cases' DataFrame:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a simple histogram:\n",
    "%matplotlib inline\n",
    "all_cases.Confirmed.plot(kind = 'hist', bins = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at growth of confirmed cases within US:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#us = covid[covid.GeneralCountry == 'US'].groupby('ObservationDate').Confirmed.max().reset_index()\n",
    "#us.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#us.plot.line(x = 'ObservationDate', y = 'Confirmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looks like we are dealing with some data error:\n",
    "#us[us.ObservationDate ==  '2020-02-27']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total cases by region within Mainland China:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a 'china_regions' DataFrame:\n",
    "#china_regions = covid[covid.Country == 'MAINLAND CHINA'].groupby('Region')[['Confirmed', 'Deaths']].max()\n",
    "#china_regions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly fancier bar plot with matplotlib:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''import matplotlib.pyplot as plt\n",
    "china_regions.sort_values(by='Confirmed',ascending=True).plot(kind='barh', figsize=(10,20)\n",
    "                                                            , color = ['#4b8bbe','lime','red']\n",
    "                                                            , width=1\n",
    "                                                            , rot=2)\n",
    "# defyning legend and titles parameters\n",
    "plt.title('Total cases by Region in Mainland China', size=20)\n",
    "plt.ylabel('Region',size=20)\n",
    "plt.yticks(size=12)\n",
    "plt.xticks(size=12)\n",
    "plt.legend(bbox_to_anchor=(0.95,0.95) # setting coordinates for the caption box\n",
    "           , frameon = True\n",
    "           , fontsize = 12\n",
    "           , ncol = 2 \n",
    "           , fancybox = True\n",
    "           , framealpha = 0.95\n",
    "           , shadow = True\n",
    "           , borderpad = 1);\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All confirmed cases around the world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a DataFrame for all Confirmed cases by date by Country:\n",
    "#by_date = covid.groupby(['ObservationDate', 'GeneralCountry']).Confirmed.max().reset_index()\n",
    "#by_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group by 'ObservationDate' to get total confirmed cases around the world  by date:\n",
    "#world = by_date.groupby('ObservationDate').Confirmed.sum().reset_index()\n",
    "#world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a simple line plot:\n",
    "world.plot.line(x = 'ObservationDate', y = 'Confirmed', color = 'purple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
